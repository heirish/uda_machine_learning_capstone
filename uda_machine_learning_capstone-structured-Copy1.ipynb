{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified. ./train.zip\n",
      "Found and verified. ./test.zip\n"
     ]
    }
   ],
   "source": [
    "#================1.retrieve data===========================\n",
    "import sys\n",
    "import os\n",
    "import urllib.request\n",
    "import getpass\n",
    "import requests\n",
    "\n",
    "def verify_datasize(filename, expected_bytes):\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print(\"Found and verified.\", filename)\n",
    "    else:\n",
    "        raise Exception(\"Failed to veryfile file [{}], file_size [{}], expected_size [{}].\".format(filename, statinfo.st_size, expected_bytes))\n",
    "\n",
    "        \n",
    "#Reports every 5% change in download progress\n",
    "last_percent_reported = None\n",
    "def download_progress_hook(count, blockSize, totalSize):\n",
    "    global last_percent_reported\n",
    "    percent = int(count * blockSize * 100) / totalSize\n",
    "    if last_percent_reported != percent:\n",
    "        if percent % 5 == 0:\n",
    "            sys.stdout.write(\"%s%%\" % percent)\n",
    "        else:\n",
    "            sys.stdout.write(\".\")\n",
    "        sys.stdout.flush()\n",
    "    last_percent_reported = percent\n",
    "\n",
    "    \n",
    "#download a file from url and check size\n",
    "data_root = \".\" #data saved local directory\n",
    "\n",
    "urls = [['https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/download/train.zip', 569918665],\n",
    "      ['https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/download/test.zip',284478493]]\n",
    "headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36'\n",
    "}\n",
    "\n",
    "def download_data_without_Authentication(url, expected_bytes, force=False):\n",
    "    dest_filename = os.path.join(data_root,url.split('/')[-1])\n",
    "    if force or not os.path.exists(dest_filename):\n",
    "        print(\"Attempting to download data from :\", url)\n",
    "        urllib.request.urlretrieve(url, dest_filename, download_progress_hook)\n",
    "        print(\"\\nDownload completed!\")\n",
    "    verify_datasize(dest_filename, expected_bytes)\n",
    "    return dest_filename\n",
    "\n",
    "#first you must login to Kaggle, direct to the specific competition, and accept rules competition, or post will direct to rules page.\n",
    "#eg:dogs-vs-cats's rule page is:https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/rules\n",
    "def download_data_with_Authentication(url, expected_bytes, force=False):\n",
    "    dest_filename = os.path.join(data_root, url.split('/')[-1])\n",
    "    if force or not os.path.exists(dest_filename):\n",
    "        print(\"Attempting to download data from :\", url)\n",
    "        user_name = input(\"Enter username:\")\n",
    "        pwd = getpass.getpass(\"Enter password:\")\n",
    "        authen_info = {'UserName':user_name, 'Password': pwd}\n",
    "        \n",
    "        #To go to the redirect url\n",
    "        resp = requests.get(url, headers=headers)\n",
    "        print(\"Redirected:\", resp.url)\n",
    "\n",
    "        #To login and get data\n",
    "        resp = requests.post(resp.url, data = authen_info, headers=headers,stream=True)\n",
    "        print(\"Redirected:\", resp.url)\n",
    "        print(\"Status:\", resp.status_code)\n",
    "        \n",
    "        if resp.status_code == requests.codes.ok:\n",
    "            f = open(dest_filename, 'wb')\n",
    "            for chunk in resp.iter_content(chunk_size = 512 * 1024):# Reads 512KB at a time into memory\n",
    "                if chunk: # filter out keep-alive new chunks\n",
    "                    f.write(chunk)\n",
    "            f.close()\n",
    "            print(\"\\nDownload completed!\")\n",
    "        else:\n",
    "            raise Exception(\"\\nDownload failed\")\n",
    "        \n",
    "    verify_datasize(dest_filename, expected_bytes)\n",
    "    return dest_filename\n",
    "   \n",
    "\n",
    "train_filename = download_data_with_Authentication(urls[0][0], urls[0][1])\n",
    "test_filename = download_data_with_Authentication(urls[1][0], urls[1][1])            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzip file succeed. ./train.zip\n",
      "Unzip file succeed. ./test.zip\n"
     ]
    }
   ],
   "source": [
    "#=================2.unzip file and explore image files======================\n",
    "import zipfile\n",
    "\n",
    "def unzip_file(filename):\n",
    "    file_to_unzip = zipfile.ZipFile(filename)\n",
    "    file_to_unzip.extractall()\n",
    "    file_to_unzip.close()\n",
    "    print(\"Unzip file succeed.\", filename)\n",
    "    \n",
    "unzip_file(train_filename)\n",
    "unzip_file(test_filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nrebuild_dir(dat_dir)\\nrebuild_dir(cate_dat_dir)\\ndog_counts = rearrange_data(\"dog\")\\ncat_counts = rearrange_data(\"cat\")\\nimage_properties_dog = get_image_properties(\"dog\")\\nimage_properties_cat = get_image_properties(\"cat\")\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "dat_dir = \"./trainprocess\"\n",
    "cate_dat_dir = dat_dir + \"/categorized\"\n",
    "train_dat_dir = dat_dir + \"/train\"\n",
    "valid_dat_dir = dat_dir + \"/valid\"\n",
    "categories = [\"cat\", \"dog\"]\n",
    "\n",
    "def rebuild_dir(dir):\n",
    "    if os.path.exists(dir):\n",
    "        shutil.rmtree(dir)\n",
    "    os.mkdir(dir)\n",
    "        \n",
    "def rearrange_data(category_name):\n",
    "    if category_name not in categories:\n",
    "        raise Exception(\"\\n category [{}] not exists.\", category_name)\n",
    "        \n",
    "    category_data_dir = cate_dat_dir + \"/\" + category_name\n",
    "    rebuild_dir(category_data_dir)\n",
    "    \n",
    "    train_filenames = os.listdir('./train')\n",
    "    train_category = filter(lambda x:x[:3].upper() ==category_name.upper() , train_filenames)\n",
    "    train_count = 0\n",
    "    for filename in train_category:\n",
    "        train_count += 1\n",
    "        shutil.copy(\"./train/\" + filename, category_data_dir)\n",
    "    return train_count\n",
    " \n",
    "def get_image_properties(category_name):\n",
    "    if category_name not in categories:\n",
    "        raise Exception(\"\\n category {{}] not exists.\", category_name)\n",
    "        \n",
    "    category_map = {}\n",
    "    path = cate_dat_dir + \"/\" + category_name + \"/\"\n",
    "    filenames = os.listdir(path)\n",
    "    for filename in filenames:\n",
    "        im = Image.open(path + filename)\n",
    "        category_map[filename] = [im.format, im.mode, im.width, im.height]\n",
    "    return category_map\n",
    "'''\n",
    "rebuild_dir(dat_dir)\n",
    "rebuild_dir(cate_dat_dir)\n",
    "dog_counts = rearrange_data(\"dog\")\n",
    "cat_counts = rearrange_data(\"cat\")\n",
    "image_properties_dog = get_image_properties(\"dog\")\n",
    "image_properties_cat = get_image_properties(\"cat\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_properties_dog' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f80b81e0e5cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'format'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mode'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'width'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'height'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf_dog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_properties_dog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdf_cat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_properties_cat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdf_dog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_properties_dog' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "columns = ['format', 'mode', 'width', 'height']\n",
    "\n",
    "df_dog = pd.DataFrame(image_properties_dog).transpose()\n",
    "df_cat = pd.DataFrame(image_properties_cat).transpose()\n",
    "df_dog.columns = columns\n",
    "df_cat.columns = columns\n",
    "\n",
    "print(\"\\ndogs count : [{}]. cats count:[{}]\".format(dog_counts, cat_counts))\n",
    "\n",
    "print(\"\\ndogs group by format, mode:\")\n",
    "print(df_dog.groupby(['format', 'mode'])['width'].count())\n",
    "print(\"\\ncats group by format, mode:\")\n",
    "print(df_cat.groupby(['format', 'mode'])['width'].count())\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.scatter(x=df_dog['width'], y=df_dog['height'], s=10, marker=\".\", color='red')\n",
    "plt.scatter(x=df_cat['width'], y=df_cat['height'], s=10, marker=\"*\", color='blue')\n",
    "plt.legend(['dogs','cats'], loc='center right')\n",
    "plt.xlabel('widht')\n",
    "plt.ylabel('height')\n",
    "plt.title(\"image size\")\n",
    "plt.subplots_adjust(hspace = .5)\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.boxplot([df_dog['width'], df_dog['height'], df_cat['width'], df_cat['height']], showmeans=True)\n",
    "plt.xticks([1,2,3,4], ['dog-width', 'dog-height', 'cat-width', 'cat-height'])\n",
    "#plt.subplots_adjust(hspace = .5)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\ndogs width max/min:\\n\",df_dog.loc[[df_dog['width'].idxmax(),df_dog['width'].idxmin()]])\n",
    "print(\"\\ndogs height max/min:\\n\",df_dog.loc[[df_dog['height'].idxmax(),df_dog['height'].idxmin()]])\n",
    "print(\"\\ncats width max/mai:\\n\",df_cat.loc[[df_cat['width'].idxmax(),df_cat['width'].idxmin()]])\n",
    "print(\"\\ncats height max/min:\\n\",df_cat.loc[[df_cat['height'].idxmax(),df_cat['height'].idxmin()]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#========================3.split and preprocess data================================\n",
    "import random\n",
    "\n",
    "def split_train_valid(category_name, test_percent, random_seed=None, create_link=False):\n",
    "    if category_name not in categories:\n",
    "        raise Exception(\"\\n category [{}] note exists.\", category_name)\n",
    "    if test_percent <=0 or test_percent >=1:\n",
    "        raise Exception(\"\\n test_percent must be in (0,1)\")\n",
    "        \n",
    "    source_dir = cate_dat_dir + \"/\" + category_name + \"/\"\n",
    "    dest_train_dir = train_dat_dir + \"/\" + category_name + \"/\"\n",
    "    dest_valid_dir = valid_dat_dir + \"/\" + category_name + \"/\"\n",
    "    \n",
    "    filenames = os.listdir(source_dir)\n",
    "    total_size = len(filenames)\n",
    "    test_size = int(total_size * test_percent)\n",
    "    train_size = total_size - test_size\n",
    "    \n",
    "    if not random_seed is None:\n",
    "        random.seed(random_seed)\n",
    "    random.shuffle(filenames)\n",
    "    rebuild_dir(dest_train_dir)\n",
    "    rebuild_dir(dest_valid_dir)\n",
    "    for i in range(0, total_size):\n",
    "        if i < test_size:\n",
    "            dest_dir = dest_valid_dir\n",
    "        else:\n",
    "            dest_dir = dest_train_dir\n",
    "        \n",
    "        if create_link == True:\n",
    "                os.symlink(source_dir + filenames[i], dest_dir + filenames[i])\n",
    "        else:\n",
    "                shutil.copy(source_dir + filenames[i], dest_dir)\n",
    "                \n",
    "    return train_size, test_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 images belonging to 2 classes.\n",
      "going to predict train features\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9c71026e937d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mtop_model_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./bottleneck_fc_model.h5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m ModelUtil.export_vgg16_bottleneck(image_width, image_height, perbatch,  \n\u001b[0;32m---> 30\u001b[0;31m                    train_feature_file, cate_dat_dir)\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mX_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_feature_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/udacity/uda_machine_learning_capstone/ModelUtil.py\u001b[0m in \u001b[0;36mexport_vgg16_bottleneck\u001b[0;34m(image_width, image_height, num_perbatch, train_features, train_dir)\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;31m# the predict_generator method returns the output of a model, given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[0;31m# a generator that yields batches of numpy data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m     \u001b[0mbottleneck_features_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"predict train features done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0;31m# save the output as a Numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict_generator\u001b[0;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m   2083\u001b[0m                     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2085\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2086\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2087\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1626\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_predict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1628\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1629\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#model_vgg16\n",
    "import ModelUtil\n",
    "import importlib\n",
    "importlib.reload(ModelUtil)\n",
    "from keras.layers import *\n",
    "from keras import optimizers\n",
    "\n",
    "dat_dir = \"./trainprocess\"\n",
    "cate_dat_dir = dat_dir + \"/categorized\"\n",
    "train_dat_dir = dat_dir + \"/train\"\n",
    "valid_dat_dir = dat_dir + \"/valid\"\n",
    "categories = [\"cat\", \"dog\"]\n",
    "\n",
    "#rebuild_dir(train_dat_dir)\n",
    "#rebuild_dir(valid_dat_dir)\n",
    "#dog_train_size, dog_test_size = split_train_valid(\"dog\", 0.3, 1234, False)\n",
    "#cat_train_size, cat_test_size = split_train_valid(\"cat\", 0.3, 1234, False)\n",
    "\n",
    "total_train_size = 17500 #dog_train_size + cat_train_size\n",
    "total_valid_size = 7500 #dog_test_size + cat_test_size\n",
    "\n",
    "image_width = 224\n",
    "image_height = 224\n",
    "image_size = (image_width,image_height)\n",
    "perbatch = 32\n",
    "\n",
    "train_feature_file = \"vgg16_bottleneck_train.npy\"\n",
    "top_model_weights = \"./bottleneck_fc_model.h5\"\n",
    "ModelUtil.export_vgg16_bottleneck(image_width, image_height, perbatch,  \n",
    "                   train_feature_file, cate_dat_dir)\n",
    "\n",
    "X_data = np.load(open(train_feature_file,\"rb\"))\n",
    "y_data = np.array([0] * 12500 + [1] * 12500)\n",
    "\n",
    "top_model = ModelUtil.top_model(train_shape=X_data.shape[1:])\n",
    "\n",
    "top_model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "top_model.fit(X_data, y_data,\n",
    "          nb_epoch=50, batch_size=perbatch,\n",
    "          validation_split=0.2)\n",
    "top_model.save_weights(top_model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "import ModelUtil\n",
    "import importlib\n",
    "importlib.reload(ModelUtil)\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "top_model_weights = \"./bottleneck_top_model.h5\"\n",
    "perbatch = 4\n",
    "\n",
    "with h5py.File(\"gap_VGG16.h5\", 'r') as h:\n",
    "    data = np.array(h['train'])\n",
    "    labels = np.array(h['label'])\n",
    "\n",
    "train_data, validation_data, train_labels, validation_labels = train_test_split(\n",
    "    data, labels, test_size=0.3, random_state=1234)\n",
    "\n",
    "model = ModelUtil.top_model(train_data.shape[1:])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_data, train_labels,\n",
    "          epochs=7, batch_size=perbatch,\n",
    "          validation_data=(validation_data, validation_labels))\n",
    "model.save_weights(top_model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ModelUtil\n",
    "import importlib\n",
    "importlib.reload(ModelUtil)\n",
    "from keras import applications\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "\n",
    "at_dir = \"./trainprocess\"\n",
    "cate_dat_dir = dat_dir + \"/categorized\"\n",
    "train_dat_dir = dat_dir + \"/train\"\n",
    "valid_dat_dir = dat_dir + \"/valid\"\n",
    "categories = [\"cat\", \"dog\"]\n",
    "\n",
    "#rebuild_dir(train_dat_dir)\n",
    "#rebuild_dir(valid_dat_dir)\n",
    "dog_train_size, dog_test_size = split_train_valid(\"dog\", 0.3, 1234, False)\n",
    "cat_train_size, cat_test_size = split_train_valid(\"cat\", 0.3, 1234, False)\n",
    "\n",
    "total_train_size = 17500 #dog_train_size + cat_train_size\n",
    "total_valid_size = 7500 #dog_test_size + cat_test_size\n",
    "image_width = 224\n",
    "image_height = 224\n",
    "image_size = (image_width,image_height)\n",
    "perbatch = 64\n",
    "poch_num = 150\n",
    "\n",
    "\n",
    "model = applications.VGG16(weights='imagenet', include_top=False, \n",
    "                      input_tensor=Input(shape=(image_width,image_height,3)))\n",
    "for layer in model.layers[:-2]:\n",
    "    layer.trainable = False    \n",
    "\n",
    "tune_model = Sequential()\n",
    "for layer in model.layers:\n",
    "    tune_model.add(layer)\n",
    "tune_model.add(GlobalAveragePooling2D())\n",
    "\n",
    "# add the model on top of the convolutional base\n",
    "top_model = ModelUtil.top_model(tune_model.output_shape[1:])\n",
    "top_model.load_weights(top_model_weights)\n",
    "\n",
    "tune_model.add(top_model)\n",
    "sgd = optimizers.SGD(lr=1e-3, momentum=0.9)\n",
    "tune_model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "tune_model_name = \"tune_model\"\n",
    "ModelUtil.visualize_model(tune_model, model_name=tune_model_name)\n",
    "history_tune_model = ModelUtil.train_data_earlystopping(tune_model, model_name=tune_model_name, epoch=poch_num,\n",
    "                          image_size = (image_width, image_height), num_perbatch=perbatch,\n",
    "                          train_dir=train_dat_dir, train_size=total_train_size,\n",
    "                          valid_dir=valid_dat_dir, valid_size=total_valid_size)\n",
    "ModelUtil.visualize_history(tune_model, model_name=tune_model_name)\n",
    "#ModelUtil.predict_data(tune_model, model_name=tune_model_name, \n",
    "#                image_size=(image_width, image_height), \n",
    "#               num_perbatch=perbatch)\n",
    "ModelUtil.save_model(tune_model, model_name=tune_model_name)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
